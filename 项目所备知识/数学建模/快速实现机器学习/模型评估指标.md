
先来介绍一下衡量模型结果好坏的一些指标。

### 回归问题的评估指标

![](../../../img/Pasted%20image%2020250825160540.png)

![](../../../img/Pasted%20image%2020250825160551.png)

### 分类问题的评估指标

![](../../../img/Pasted%20image%2020250825160653.png)

![](../../../img/Pasted%20image%2020250825161001.png)

在此之前，我们需要==定义分类结果中的正类（positive）和负类（negative）==。这里的正类和负类实际上借用了医学中的阳性（positive）和阴性（negative）的概念，医学中一般阴性代表正常，而阳性则代表患有疾病。在机器学习中，我们通常将==更关注的事件==定义为正类事件。（一般将坏的情况作为正类事件）

例如上面的西瓜分类的例子中，如果我们==更关注坏瓜，就定义坏瓜为正类==，好瓜为负类。（有些地方也用 0 和 1 表示分类结果，一般正类记为 1，负类记为 0）。

（这种划分正类和负类的标准也不是绝对的，如果你更关注好瓜，那么你也可以把好瓜定义成正类；另外有时候我们很难去区分结果的好坏，例如我们要对猫和狗的图片进行分类，这时候正类和负类无论怎么定义都行）

![](../../../img/Pasted%20image%2020250825161605.png)


## 评估指标

![](../../../img/Pasted%20image%2020250825161644.png)

通常来说，查全率和查准率是负相关关系的。

![](../../../img/Pasted%20image%2020250825161947.png)

如何权衡查全率和查准率？

![](../../../img/Pasted%20image%2020250825162443.png)

### ROC曲线和AUC

![](../../../img/Pasted%20image%2020250825162746.png)

下图就是一个ROC曲线的例子。
![](../../../img/Pasted%20image%2020250825162803.png)

（1）将不同的模型的 ROC 曲线绘制在同一张图内，==最靠近左上角的那条曲线代表的模型的分类效果最好。==
（2）实际任务中，情况很复杂，如果两条 ROC 曲线发生了==交叉==，则很难一般性地断言谁优谁劣。因此我们引入 AUC，AUC（Area Under Curve）被定义为 ==ROC 曲线与下方的坐标轴围成的面积==，AUC 的范围位于`[0,1]`之间，==AUC 越大则模型的分类效果越好==，==如果 AUC 小于等于 0.5，则该模型是不能用的。通常 AUC 大于 0.85 的模型就表现可以了。==


### 模型的泛化能力

模型的泛化能力(generalization ability)是指由该模型对未知数据的预测能力。

还是举西瓜分类的例子，我们利用 100 个西瓜的数据建立了一个模型，不妨称其为“西瓜分类器”，它能帮助我们区分好瓜和坏瓜。
那么我们怎样去评价这个“西瓜分类器”好不好用呢？
有同学会想，前面我们学了那么多用来评价分类结果好坏的评估指标，比如分类准确率、F1 分数、AUC 等。如果这些指标都很大的话不就说明我们的模型很好吗？

事实上这是不够的，==我们更加关心的是这个“西瓜分类器”在面对一组新的数据的时候，它的分类能力还是不是足够好。==
打个比方，准备期末考试的时候，大家都做课后作业题复习，可能你每道课后题都会做，但上了考场还是啥都不会。

![](../../../img/Pasted%20image%2020250826111651.png)

### 留出法

前面我们为了衡量“西瓜分类器”的泛化能力，我们又买了 20 个西瓜用于测试。但==大多少时候我们获取新的测试样本的成本较高==，例如银行判断申请贷款的客户是否会违约时，需要等到客户还钱的时候才知道结果。因此，我们需要想一个办法，只==使用已有的样本数据来对模型的泛化能力进行一个评价==。

![](../../../img/Pasted%20image%2020250826113433.png)

==我们将这里的 80 个西瓜称为**训练集(train set)**，它们用来训练我们的模型，得到我们模型中的待估参数；剩下的 20 个西瓜我们不参与模型的训练过程，只用来最后对模型的好坏进行测试，因此被称为**测试集(test set)**。我们将上面这种对泛化能力进行评估的方法称为留出法（Hold-Out）。==

有以下几点要说明：

（1） 假设我们总共的样本量为 N，我们要将其划分为训练集和测试集，这两个集合的划分比例通常设置为：6:4、7:3 或 8:2。（一般为7：3）

（2） 训练集和测试集的划分==既要随机，又要尽可能保持数据分布的一致性==（在分类问题中就是类别比例的相似），例如原来 100 个瓜中有 60 个好瓜，40个坏瓜，那么你按照 8:2 的比例生成训练集和测试集时，尽量保证测试集中的 20 个样本内有 12 个好瓜和 8 个坏瓜。在分类任务中，==保留类别比例的采样方法称为分层采样==（stratified sampling）。

在留出法中，用于评价模型泛化能力的测试集只是所有样本的一部分，而且这个结果不是很稳定，对模型的泛化能力的评价依赖于哪些样本点落入训练集，哪些样本点在测试集。

### 交叉验证（k 折交叉验证）

我们先将数据集 D 随机的划分为 k 个大小相似的==互斥==子集。==每一次用 k-1 个子集的并集作为训练集，剩下的一个子集作为测试集==；这样就可以获得 k 组训练/测试集，从而可进行 k 次训练和测试，最终返回的是这 k 次测试的平均结果，通常 k 取 10，此时称为 10 折交叉验证。

![](../../../img/Pasted%20image%2020250826130728.png)

![](../../../img/Pasted%20image%2020250826130741.png)

![](../../../img/Pasted%20image%2020250828191049.png)

### 选择最好的模型

对于同一个问题，我们可以建立不同的模型去解决。例如前面介绍的西瓜分类问题中，我们可以使用决策树、K 最近邻（KNN）、支持向量机（SVM）等常用的机器学习模型。那么，我们应该怎样衡量一个模型的好坏呢？

我们前面介绍了留出法和交叉验证法，这里面都需要将数据分成训练集和测试集。因此，==我们可以在同一个训练集下，分别对这些模型进行训练，然后将这些模型分别在测试集上进行预测，并比较不同模型的泛化能力，我们选择泛化能力最好的模型。==（该模型在测试集上的表现最好，例如误差最小，具体的评价指标在前面有介绍）

另外，大多数的模型中都需要设定一些参数(parameter)，参数不同得到的结果可能有很明显的差异。因此，除了要对模型进行选择外，还需要对模型中的==参数进行设定==，这就是机器学习中常说的“参数调节”或简称“调参”(parameter tuning)。通常调参依赖于经验，我们后面会介绍==网格搜索==的方法，来自动搜索使模型效果最好的参数。

![](../../../img/Pasted%20image%2020250826131618.png)


### 欠拟合和过拟合

过拟合（overfitting）指的是模型在==训练集上表现的很好==，但是==在测试集上表现的并不理想==，也就是说模型对未知样本的预测表现一般，泛化能力较差。

如果模型不仅在训练数据集上的==预测结果不好==，而且在==测试数据集上的表现也不理想==，也就是说两者的表现都很糟糕，那么我们有理由怀疑模型发生了==欠拟合==（underfitting）现象。

![](../../../img/Pasted%20image%2020250826132005.png)

![](../../../img/Pasted%20image%2020250826131948.png)

![](../../../img/Pasted%20image%2020250826132422.png)

解决过拟合的方法：

（1） 通过前面介绍的==交叉验证==的方法来选择合适的模型，并==对参数进行调节==。

（2） ==扩大样本数量、训练更多的数据==。

（3） ==对模型中的参数增加正则化==。（即增加惩罚项，参数越多惩罚越大）

欠拟合则和过拟合刚好相反，我们可以==增加模型的参数、或者选择更加复杂的模型==；也可以==从数据中挖掘更多的特征来增加输入的变量==，还可以使用一些==集成算法==（如装袋法（Bagging），提升法（Boosting））。
（注意：有可能模型的输入和输出一点关系都没有，举个极端的例子，你买的西瓜好坏和你的个人特征没任何关系，例如你的性别身高体重等）

![](../../../img/Pasted%20image%2020250826134734.png)

