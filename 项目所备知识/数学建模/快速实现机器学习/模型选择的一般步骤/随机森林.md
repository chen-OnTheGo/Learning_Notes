
随机森林可以给我们估计输入指标的重要性，可以帮助我们进行进行特征选择筛选出无关的特征。

![](../../../../img/Pasted%20image%2020250829223256.png)

（若比赛中用到了随机森林，可以将上方的图重新画一下放到论文里面，用于解释随机森林建模原理）

本文介绍随机森林的三个作用：
- 利用随机森林得到指标的重要性
- [利用随机森林得到样本属于某一类的概率](随机森林.md#利用随机森林得到样本属于某一类的概率)
- 
### 利用随机森林得到指标的重要性

![](../../../../img/Pasted%20image%2020250829225837.png)

推荐一篇论文：https://www.doc88.com/p-5416374866370.html

#### 什么是袋外数据（OOB）

![](../../../../img/Pasted%20image%2020250829230015.png)

![](../../../../img/Pasted%20image%2020250829230034.png)

如果自变量中有分类变量的话，得到的指标重要性的结果可能是有偏的，我们需要更改决策树中的设置：（后面泰坦尼克号的例题里面我们会用到这一点）

![](../../../../img/Pasted%20image%2020250829233301.png)


### 特征选择

（若比赛的题目中没有明确指定要从所有的自变量中筛选出重要的变量，往往可以不进行特征选择；或者说题目中给的一些指标没有用，去除这些不重要指标后模型的精确读会大大提高，此时可以进行特征选择）

特征选择” (feature selection)是从数据集的诸多特征里面选择和目标变量相关的特征，去掉那些不相关的特征。

![](../../../../img/Pasted%20image%2020250830162313.png)

#### 特征选择的作用

（1）可以==缓解维数灾难==（Curse of Dimensionality），对于某些模型例如 K 最近邻（KNN）算法，当指标个数太多时，样本在空间中的分布越呈现稀疏性，模型最后得到的效果会大打折扣。维数灾难的介绍请在知乎搜索：《怎样理解 Curse of Dimensionality（维数灾难）?》

（2）==降低学习任务的难度，减少训练的时间。==

![](../../../../img/Pasted%20image%2020250830170214.png)

若题目让我们解释最后的结果，就只能用特征选择（PCA的结果不太好解释）


### 利用随机森林得到样本属于某一类的概率

![](../../../../img/Pasted%20image%2020250830170731.png)

![](../../../../img/Pasted%20image%2020250830170759.png)


若遇到样本不平衡问题，随机森林的效果就会比较差，这时候我们就需要用[最大相关最小冗余算法（mRMR）](../最大相关最小冗余算法（mRMR）.md)。